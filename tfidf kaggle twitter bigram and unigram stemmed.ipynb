{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NATURAL LANGUAGE PROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just a preliminary/draft code for the model:\n",
    "Cleaning twitter data and classifying them as positive negative or Neutral sentiments\n",
    "\n",
    "Coded by: Easter Joy Trocio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error 3] The system cannot find the path specified: u'C:/Users/Joy/Documents'\n",
      "c:\\users\\jgcty\\downloads\n"
     ]
    }
   ],
   "source": [
    "cd C:\\Users\\Joy\\Documents\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Airline Kaggle Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "K=pd.read_csv('Tweets_Airline_Kaggle_Data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.700000e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.700000e+17</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.700000e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.700000e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.700000e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  5.700000e+17           neutral                        1.0000   \n",
       "1  5.700000e+17          positive                        0.3486   \n",
       "2  5.700000e+17           neutral                        0.6837   \n",
       "3  5.700000e+17          negative                        1.0000   \n",
       "4  5.700000e+17          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T1=K[K.airline_sentiment== 'positive']\n",
    "#T2=K[K.airline_sentiment== 'negative']\n",
    "T3=K[K.airline_sentiment== 'neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Kaggle= [T1[:1000],T3[:1000]]\n",
    "Kaggle = pd.concat(Kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Kaggle=Kaggle[['airline_sentiment','text']]\n",
    "Kaggle.columns=['Senti_cess_1','message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Senti_cess_1</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica yes, nearly every time I fly VX...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>positive</td>\n",
       "      <td>@virginamerica Well, I didn't�but NOW I DO! :-D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica it was amazing, and arrived an ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica I &amp;lt;3 pretty graphics. so muc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Senti_cess_1                                            message\n",
       "1      positive  @VirginAmerica plus you've added commercials t...\n",
       "6      positive  @VirginAmerica yes, nearly every time I fly VX...\n",
       "8      positive    @virginamerica Well, I didn't�but NOW I DO! :-D\n",
       "9      positive  @VirginAmerica it was amazing, and arrived an ...\n",
       "11     positive  @VirginAmerica I &lt;3 pretty graphics. so muc..."
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Kaggle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Kaggle.loc[Kaggle.Senti_cess_1 == 'positive', 'Senti_cess'] ='pos'\n",
    "Kaggle.loc[Kaggle.Senti_cess_1 == 'neutral', 'Senti_cess'] ='neut'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Kagg=Kaggle[['Senti_cess','message']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Senti_cess</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4771</th>\n",
       "      <td>neut</td>\n",
       "      <td>@SouthwestAir arrangements to reimburse me for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4778</th>\n",
       "      <td>neut</td>\n",
       "      <td>@SouthwestAir got an email confirmation of wif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4780</th>\n",
       "      <td>neut</td>\n",
       "      <td>@SouthwestAir Flight 3744 (N284WN) departs @Fl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4782</th>\n",
       "      <td>neut</td>\n",
       "      <td>@SouthwestAir   Are flights going into Dallas ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4784</th>\n",
       "      <td>neut</td>\n",
       "      <td>@SouthwestAir had to be stuck in the middle se...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Senti_cess                                            message\n",
       "4771       neut  @SouthwestAir arrangements to reimburse me for...\n",
       "4778       neut  @SouthwestAir got an email confirmation of wif...\n",
       "4780       neut  @SouthwestAir Flight 3744 (N284WN) departs @Fl...\n",
       "4782       neut  @SouthwestAir   Are flights going into Dallas ...\n",
       "4784       neut  @SouthwestAir had to be stuck in the middle se..."
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Kagg.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from collections import Counter\n",
    "#Counter(\" \".join(Kagg['message']).split()).most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "s=pd.read_csv('TwitterData_Append_deleteNan.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "s=s.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2052"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res=s[['Senti_cess','message']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Senti_cess</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neg</td>\n",
       "      <td>@BurgerKingUK sort your huddersfield town cent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neg</td>\n",
       "      <td>@BurgerKingUK would just like to say how awful...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neg</td>\n",
       "      <td>Dear @BurgerKingUK, every visit to your Westfi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neg</td>\n",
       "      <td>@KFC_UKI we have just had a big order from you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neg</td>\n",
       "      <td>@BurgerKingUK why are bean burgers soggy &amp;amp;...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Senti_cess                                            message\n",
       "0        neg  @BurgerKingUK sort your huddersfield town cent...\n",
       "1        neg  @BurgerKingUK would just like to say how awful...\n",
       "2        neg  Dear @BurgerKingUK, every visit to your Westfi...\n",
       "3        neg  @KFC_UKI we have just had a big order from you...\n",
       "4        neg  @BurgerKingUK why are bean burgers soggy &amp;..."
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Append Twitter and Kaggle Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = [res,Kagg]\n",
    "res1 = pd.concat(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4052"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "res1=res1.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joy\\Documents\\AADavid\n"
     ]
    }
   ],
   "source": [
    "cd C:\\Users\\Joy\\Documents\\AADavid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Here for reading Twitter + Kaggle Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "res1=pd.read_csv('TwitterData_Append_KaggleData_shuffle_DeleteNAN_.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "res1=res1.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#testk=pd.read_csv('newtest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#testk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#res1=res1.append(testk).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4052"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLEANING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete punctuations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from collections import Counter\n",
    "#result2 = Counter(\" \".join(res1['message'].values.tolist()).split(\" \")).most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "res1['clean_pun']=res1['message'].str.split().apply(lambda x: [re.sub(r'[^a-zA-Z0-9 ]',r'',item) for item in x ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res1['clean_num']=res1['clean_pun'].apply(lambda x: [re.sub(r'[0-9]+',r'',item) for item in x ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exclude Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exclude = ['united','SouthwestAir','VirginAmerica','flights','&amp;','kfc','flying','KFCUKI','DM','United','BurgerKingUK','KFC_UKI', 'McDonaldsUK', '??', '&amp;', 'KFC', '????','McDonalds','-','burger','meal','chicken']\n",
    "#res1[\"clean_exclude\"] = res1[\"message\"].str.split()\n",
    "#res1[\"clean_exclude\"] =res1['message']\n",
    "res1['clean_exclude']=res1['clean_num'].apply(lambda x: [item for item in x if item not in exclude])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res1['clean_exclude_stop']=res1['clean_exclude'].apply(lambda x: [item.lower()  for item in x if item not in stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "#list(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#res1['clean_exclude_stop_punct']=res1['clean_exclude_stop'].apply(lambda x: [item for item in x if item not in string.punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#a=Counter(str([\" \".join(res1['clean_exclude_stop_punct'][i]) for i in range(len(res1['clean_exclude_stop_punct']))]).split(\" \")).most_common(2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete words that starts with @"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#res1['cleaned_@'] = res1['clean_exclude_stop_punct'].apply(lambda x: [word for word in x if not word.startswith('@')]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete words that starts with # https"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#res1['cleaned_@#'] = res1['cleaned_@'].apply(lambda x: [word for word in x if not word.startswith('#')]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res1['cleaned_@#ht'] = res1['clean_exclude_stop'].apply(lambda x: [word for word in x if not word.startswith('http')]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#res1['cleaned_@#ht\\x'] = res1['cleaned_@#ht'].apply(lambda x: [word for word in x if not word.contains('\\x')]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res1['clean_exclude_stop_punct_@#ht']=res1['cleaned_@#ht']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4052"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import csv\n",
    "#res1.to_csv('TwitterData_Append_shuffle_Cleaned.csv')\n",
    "len(res1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "#result2 = Counter(\" \".join(res1['Senti_cess'].values.tolist()).split(\" \")).items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "please\n",
      "follow\n",
      "reference\n",
      "recent\n",
      "purchase\n",
      "\n",
      "thanks\n"
     ]
    }
   ],
   "source": [
    "for i in res1[\"clean_exclude_stop_punct_@#ht\"][1]:\n",
    "    print i\n",
    "#stemmer.stem('i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res1['stemmed'] = res1[\"clean_exclude_stop_punct_@#ht\"].apply(lambda x: [stemmer.stem(y) for y in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Done Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "#r=pd.read_csv('TwitterData_Append_KaggleData_shuffle_DeleteNAN_cleaned_@#http.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r=res1[['Senti_cess','stemmed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r.columns=['sentiment','data1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "str_data=[]\n",
    "for i in r['data1']:\n",
    "    str_data.append(' '.join(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joy\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\ipykernel\\__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "r['data']=str_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>data1</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4047</th>\n",
       "      <td>pos</td>\n",
       "      <td>[deborah, help]</td>\n",
       "      <td>deborah help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4048</th>\n",
       "      <td>neut</td>\n",
       "      <td>[hi, , , piec, still, when, offer, actual, end, ]</td>\n",
       "      <td>hi   piec still when offer actual end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4049</th>\n",
       "      <td>neut</td>\n",
       "      <td>[need, wheelchair, gate, sure, what, option]</td>\n",
       "      <td>need wheelchair gate sure what option</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4050</th>\n",
       "      <td>neg</td>\n",
       "      <td>[realli, still, leav, shit, bag, what, point, ...</td>\n",
       "      <td>realli still leav shit bag what point sell u d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4051</th>\n",
       "      <td>pos</td>\n",
       "      <td>[thank, , great, servic, staff, let, chang, fl...</td>\n",
       "      <td>thank  great servic staff let chang flight  ti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentiment                                              data1  \\\n",
       "4047       pos                                    [deborah, help]   \n",
       "4048      neut  [hi, , , piec, still, when, offer, actual, end, ]   \n",
       "4049      neut       [need, wheelchair, gate, sure, what, option]   \n",
       "4050       neg  [realli, still, leav, shit, bag, what, point, ...   \n",
       "4051       pos  [thank, , great, servic, staff, let, chang, fl...   \n",
       "\n",
       "                                                   data  \n",
       "4047                                       deborah help  \n",
       "4048             hi   piec still when offer actual end   \n",
       "4049              need wheelchair gate sure what option  \n",
       "4050  realli still leav shit bag what point sell u d...  \n",
       "4051  thank  great servic staff let chang flight  ti...  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4052"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean=[]\n",
    "import string\n",
    "printable = set(string.printable)\n",
    "for s in r['data']:\n",
    "    clean.append(filter(lambda x: x in printable,s ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4052"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joy\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\ipykernel\\__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "r['data']=clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r=r[['sentiment','data']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r.columns=['target','data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Preparing for Modelling"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#http://radimrehurek.com/data_science_python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUt the Data for the New Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#newtest=r[4052:]\n",
    "#r=r[:4052]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4052</th>\n",
       "      <td>NaN</td>\n",
       "      <td>mani messag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4053</th>\n",
       "      <td>NaN</td>\n",
       "      <td>veuta websit problem login</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4054</th>\n",
       "      <td>NaN</td>\n",
       "      <td>impress veuta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4055</th>\n",
       "      <td>NaN</td>\n",
       "      <td>veuta suck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4056</th>\n",
       "      <td>NaN</td>\n",
       "      <td>respond email asap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4057</th>\n",
       "      <td>NaN</td>\n",
       "      <td>autom messag ai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4058</th>\n",
       "      <td>NaN</td>\n",
       "      <td>text analyt social media</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4059</th>\n",
       "      <td>NaN</td>\n",
       "      <td>data scienc big data</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     target                        data\n",
       "4052    NaN                 mani messag\n",
       "4053    NaN  veuta websit problem login\n",
       "4054    NaN               impress veuta\n",
       "4055    NaN                  veuta suck\n",
       "4056    NaN          respond email asap\n",
       "4057    NaN             autom messag ai\n",
       "4058    NaN    text analyt social media\n",
       "4059    NaN        data scienc big data"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#newtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r=r.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 3039 instances, test on 1013 instances\n"
     ]
    }
   ],
   "source": [
    "cutoff = len(r['data'])*3/4\n",
    "\n",
    "train=r[:cutoff]\n",
    "\n",
    "test=r[cutoff:].reset_index()\n",
    "#test=newtest.reset_index()\n",
    "print 'train on %d instances, test on %d instances' % (len(train), len(test))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = r['target'][:cutoff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#y_test= r['target'][cutoff:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3039, 4885)\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing text with scikit-learn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#bigram and unigram: count_vector = CountVectorizer(ngram_range=(2, 2))\n",
    "count_vector = CountVectorizer()\n",
    "x_train_counts = count_vector.fit_transform(train['data'])\n",
    "\n",
    "# Dimensions of the training data count vector\n",
    "print x_train_counts.shape\n",
    "\n",
    "\n",
    "# Get index of some common words/n-grams/consecutive characters\n",
    "# For example: 'movie'\n",
    "count_vector.vocabulary_.get(u'movie')\n",
    "\n",
    "\n",
    "# Get feature names\n",
    "#count_vector.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'the doctor': 5, u'keeps the': 4, u'doctor away': 3, u'apple day': 1, u'day keeps': 2, u'an apple': 0}\n"
     ]
    }
   ],
   "source": [
    "print(count_vector.fit([\"an apple a day keeps the doctor away\"]).vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Converting occurrences to frequencies\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "## Term Frequencies (tf)\n",
    "# Use fit() method to fit estimator to the data\n",
    "tf_transformer = TfidfTransformer(use_idf = False).fit(x_train_counts)\n",
    "# Use transform() method to transform count-matrix to 'tf' representation\n",
    "x_train_tf = tf_transformer.transform(x_train_counts)\n",
    "\n",
    "## Term Frequency times Inverse Document Frequency (tf-idf)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "# Use transform() method to transform count-matrix to 'tf-idf' representation\n",
    "x_train_tfidf = tfidf_transformer.fit_transform(x_train_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb= MultinomialNB()\n",
    "gnb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#X.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>target</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4052</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mani messag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4053</td>\n",
       "      <td>NaN</td>\n",
       "      <td>veuta websit problem login</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4054</td>\n",
       "      <td>NaN</td>\n",
       "      <td>impress veuta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4055</td>\n",
       "      <td>NaN</td>\n",
       "      <td>veuta suck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4056</td>\n",
       "      <td>NaN</td>\n",
       "      <td>respond email asap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4057</td>\n",
       "      <td>NaN</td>\n",
       "      <td>autom messag ai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4058</td>\n",
       "      <td>NaN</td>\n",
       "      <td>text analyt social media</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4059</td>\n",
       "      <td>NaN</td>\n",
       "      <td>data scienc big data</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index target                        data\n",
       "0   4052    NaN                 mani messag\n",
       "1   4053    NaN  veuta websit problem login\n",
       "2   4054    NaN               impress veuta\n",
       "3   4055    NaN                  veuta suck\n",
       "4   4056    NaN          respond email asap\n",
       "5   4057    NaN             autom messag ai\n",
       "6   4058    NaN    text analyt social media\n",
       "7   4059    NaN        data scienc big data"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import svm\n",
    "clf=svm.LinearSVC()\n",
    "\n",
    "V = clf.fit(x_train_tfidf.toarray(), train['target'])\n",
    "\n",
    "## Prediction on test data\n",
    "# Tokenizing test phrase\n",
    "x_test_counts = count_vector.transform(test['data'])\n",
    "# Use transform() method to transform test count-matrix to 'tf-idf' representation\n",
    "x_test_tfidf = tfidf_transformer.transform(x_test_counts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prediction\n",
    "predicted = V.predict(x_test_tfidf.toarray())\n",
    "test['predicted']=predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#V.predict_proba(x_test_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "for i in test['predicted']:\n",
    "    dist =clf.classifier.prob_classify(x_test_tfidf.toarray())\n",
    "    print dist.prob(\"pos\")\n",
    "    print dist.prob(\"neut\")\n",
    "    print dist.prob(\"neg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     |       n     |\n",
      "     |   n   e   p |\n",
      "     |   e   u   o |\n",
      "     |   g   t   s |\n",
      "-----+-------------+\n",
      " neg |<372> 37   9 |\n",
      "neut |  67<185> 51 |\n",
      " pos |  26  56<210>|\n",
      "-----+-------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "testres = []\n",
    "predres = []\n",
    "for i in range(len(test)):\n",
    "    testres.append(test['target'][i])\n",
    "for i in range(len(test)):\n",
    "    predres.append(test['predicted'][i])\n",
    "from nltk.metrics import ConfusionMatrix\n",
    "cm = nltk.ConfusionMatrix(testres, predres)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5844027640671273"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(379+92+121)*1.00/len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('stemmed:', 0.7393879565646594)"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'stemmed:',(364+203+182)*1.00/len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('stemmed: bigram +unigram', 0.7561697926949654)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'stemmed: bigram +unigram',(384+196+186)*1.00/len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.80      0.89      0.84       418\n",
      "       neut       0.67      0.61      0.64       303\n",
      "        pos       0.78      0.72      0.75       292\n",
      "\n",
      "avg / total       0.75      0.76      0.75      1013\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(testres, predres))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.SGDClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "SG = clf.fit(x_train_tfidf.toarray(), train['target'])\n",
    "\n",
    "## Prediction on test data\n",
    "# Tokenizing test phrase\n",
    "x_test_counts = count_vector.transform(test['data'])\n",
    "# Use transform() method to transform test count-matrix to 'tf-idf' representation\n",
    "x_test_tfidf = tfidf_transformer.transform(x_test_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prediction\n",
    "predicted = SG.predict(x_test_tfidf.toarray())\n",
    "test['predicted']=predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     |       n     |\n",
      "     |   n   e   p |\n",
      "     |   e   u   o |\n",
      "     |   g   t   s |\n",
      "-----+-------------+\n",
      " neg |<335> 48  35 |\n",
      "neut |  62<147> 94 |\n",
      " pos |  17  34<241>|\n",
      "-----+-------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "testres = []\n",
    "predres = []\n",
    "for i in range(len(test)):\n",
    "    testres.append(test['target'][i])\n",
    "for i in range(len(test)):\n",
    "    predres.append(test['predicted'][i])\n",
    "from nltk.metrics import ConfusionMatrix\n",
    "cm = nltk.ConfusionMatrix(testres, predres)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7295162882527148"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(387+191+161)*1.00/len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('stemmed:', 0.7305034550839091)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'stemmed:',(375+219+146)*1.00/len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('stemmed: bigram +unigram', 0.7423494570582428)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'stemmed: bigram +unigram',(370+179+203)*1.00/len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.81      0.80      0.81       418\n",
      "       neut       0.64      0.49      0.55       303\n",
      "        pos       0.65      0.83      0.73       292\n",
      "\n",
      "avg / total       0.71      0.71      0.71      1013\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(testres, predres))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Training a classifier to predict sentiment label of a phrase\n",
    "# Naive Bayes Classifier (Multinomial)\n",
    "#from sklearn.naive_bayes import MultinominalNB\n",
    "G = GaussianNB().fit(x_train_tfidf.toarray(), train['target'])\n",
    "\n",
    "## Prediction on test data\n",
    "# Tokenizing test phrase\n",
    "x_test_counts = count_vector.transform(test['data'])\n",
    "# Use transform() method to transform test count-matrix to 'tf-idf' representation\n",
    "x_test_tfidf = tfidf_transformer.transform(x_test_counts)\n",
    "\n",
    "# Prediction\n",
    "predicted = G.predict(x_test_tfidf.toarray())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1013"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# View predictions\n",
    "test['predicted']=predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>target</th>\n",
       "      <th>data</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3039</td>\n",
       "      <td>pos</td>\n",
       "      <td>resolv over hour work ground amp somehow syste...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3040</td>\n",
       "      <td>neg</td>\n",
       "      <td>won tv  month pass mcdonald monopolylif look</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3041</td>\n",
       "      <td>pos</td>\n",
       "      <td>oh gosh go dm thank</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3042</td>\n",
       "      <td>pos</td>\n",
       "      <td>sorri i end us air flight thank though get back</td>\n",
       "      <td>neut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3043</td>\n",
       "      <td>neut</td>\n",
       "      <td>bruh giannile turn vxsafetyd</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index target                                               data predicted\n",
       "0   3039    pos  resolv over hour work ground amp somehow syste...       pos\n",
       "1   3040    neg       won tv  month pass mcdonald monopolylif look       pos\n",
       "2   3041    pos                                oh gosh go dm thank       pos\n",
       "3   3042    pos    sorri i end us air flight thank though get back      neut\n",
       "4   3043   neut                       bruh giannile turn vxsafetyd       pos"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5616979269496545"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(252+145+172)*1.00/len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('stemmed:', 0.7305034550839091)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'stemmed:',(375+219+146)*1.00/len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('stemmed: bigram +unigram', 0.6001974333662389)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'stemmed: bigram +unigram', (302+139+167)*1.00/len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     |       n     |\n",
      "     |   n   e   p |\n",
      "     |   e   u   o |\n",
      "     |   g   t   s |\n",
      "-----+-------------+\n",
      " neg |<302> 63  51 |\n",
      "neut |  65<139>142 |\n",
      " pos |  35  49<167>|\n",
      "-----+-------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testres = []\n",
    "predres = []\n",
    "for i in range(len(test)):\n",
    "    testres.append(test['target'][i])\n",
    "for i in range(len(test)):\n",
    "    predres.append(test['predicted'][i])\n",
    "from nltk.metrics import ConfusionMatrix\n",
    "cm = nltk.ConfusionMatrix(testres, predres)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Training a classifier to predict sentiment label of a phrase\n",
    "# Naive Bayes Classifier (Multinomial)\n",
    "#from sklearn.naive_bayes import MultinominalNB\n",
    "M = MultinomialNB().fit(x_train_tfidf.toarray(), train['target'])\n",
    "\n",
    "## Prediction on test data\n",
    "# Tokenizing test phrase\n",
    "x_test_counts = count_vector.transform(test['data'])\n",
    "# Use transform() method to transform test count-matrix to 'tf-idf' representation\n",
    "x_test_tfidf = tfidf_transformer.transform(x_test_counts)\n",
    "\n",
    "# Prediction\n",
    "predicted2 = M.predict(x_test_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# View predictions\n",
    "test['predicted_M']=predicted2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     |       n     |\n",
      "     |   n   e   p |\n",
      "     |   e   u   o |\n",
      "     |   g   t   s |\n",
      "-----+-------------+\n",
      " neg |<407>  4   5 |\n",
      "neut | 131<168> 47 |\n",
      " pos |  52  34<165>|\n",
      "-----+-------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testres = []\n",
    "predres = []\n",
    "for i in range(len(test)):\n",
    "    testres.append(test['target'][i])\n",
    "for i in range(len(test)):\n",
    "    predres.append(test['predicted_M'][i])\n",
    "from nltk.metrics import ConfusionMatrix\n",
    "cm = nltk.ConfusionMatrix(testres, predres)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7403751233958539"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(399+175+176)*1.00/len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('stemmed:', 0.7413622902270484)"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'stemmed:',(398+178+175)*1.00/len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('stemmed: bigram +unigram', 0.7305034550839091)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'stemmed: bigram +unigram', (407+168+165)*1.00/len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BernoulliNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Training a classifier to predict sentiment label of a phrase\n",
    "# Naive Bayes Classifier (Multinomial)\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "B = BernoulliNB().fit(x_train_tfidf.toarray(), train['target'])\n",
    "\n",
    "## Prediction on test data\n",
    "# Tokenizing test phrase\n",
    "x_test_counts = count_vector.transform(test['data'])\n",
    "# Use transform() method to transform test count-matrix to 'tf-idf' representation\n",
    "x_test_tfidf = tfidf_transformer.transform(x_test_counts)\n",
    "\n",
    "# Prediction\n",
    "predicted3 = B.predict(x_test_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     |       n     |\n",
      "     |   n   e   p |\n",
      "     |   e   u   o |\n",
      "     |   g   t   s |\n",
      "-----+-------------+\n",
      " neg |<412>  4   . |\n",
      "neut | 182<156>  8 |\n",
      " pos | 106  51 <94>|\n",
      "-----+-------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "0.757156959526\n",
      "stemmed: 0.764067127345\n",
      "stemmed: bigram +unigram 0.653504442251\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "predicted3 = B.predict(x_test_tfidf.toarray())\n",
    "\n",
    "# View predictions\n",
    "test['predicted_B']=predicted3\n",
    "\n",
    "testres = []\n",
    "predres = []\n",
    "for i in range(len(test)):\n",
    "    testres.append(test['target'][i])\n",
    "for i in range(len(test)):\n",
    "    predres.append(test['predicted_B'][i])\n",
    "from nltk.metrics import ConfusionMatrix\n",
    "cm = nltk.ConfusionMatrix(testres, predres)\n",
    "print(cm)\n",
    "\n",
    "print (383+214+170)*1.00/len(test)\n",
    "print 'stemmed:',(381+217+176)*1.00/len(test)\n",
    "print 'stemmed: bigram +unigram', (412+156+94)*1.00/len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest Centriod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joy\\Documents\\AADavid\\tfidf\n"
     ]
    }
   ],
   "source": [
    "cd C:\\Users\\Joy\\Documents\\AADavid\\tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Training a classifier to predict sentiment label of a phrase\n",
    "# Naive Bayes Classifier (Multinomial)\n",
    "#from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "clf = NearestCentroid()\n",
    "\n",
    "\n",
    "NN = clf.fit(x_train_tfidf.toarray(), train['target'])\n",
    "\n",
    "## Prediction on test data\n",
    "# Tokenizing test phrase\n",
    "x_test_counts = count_vector.transform(test['data'])\n",
    "# Use transform() method to transform test count-matrix to 'tf-idf' representation\n",
    "x_test_tfidf = tfidf_transformer.transform(x_test_counts)\n",
    "\n",
    "# Prediction\n",
    "predicted4 = NN.predict(x_test_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     |       n     |\n",
      "     |   n   e   p |\n",
      "     |   e   u   o |\n",
      "     |   g   t   s |\n",
      "-----+-------------+\n",
      " neg |<352> 52  12 |\n",
      "neut |  57<269> 20 |\n",
      " pos |  30 114<107>|\n",
      "-----+-------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "0.731490621915\n",
      "0.71865745311\n"
     ]
    }
   ],
   "source": [
    "# View predictions\n",
    "test['predicted_NC']=predicted4\n",
    "\n",
    "testres = []\n",
    "predres = []\n",
    "for i in range(len(test)):\n",
    "    testres.append(test['target'][i])\n",
    "for i in range(len(test)):\n",
    "    predres.append(test['predicted_NC'][i])\n",
    "from nltk.metrics import ConfusionMatrix\n",
    "cm = nltk.ConfusionMatrix(testres, predres)\n",
    "print(cm)\n",
    "\n",
    "print (347+267+127)*1.00/len(test)\n",
    "print 'stemmed:',(352+269+107)*1.00/len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Training a classifier to predict sentiment label of a phrase\n",
    "# Naive Bayes Classifier (Multinomial)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "S = RandomForestClassifier(n_estimators=10)\n",
    "\n",
    "NN = S.fit(x_train_tfidf.toarray(), train['target'])\n",
    "\n",
    "## Prediction on test data\n",
    "# Tokenizing test phrase\n",
    "x_test_counts = count_vector.transform(test['data'])\n",
    "# Use transform() method to transform test count-matrix to 'tf-idf' representation\n",
    "x_test_tfidf = tfidf_transformer.transform(x_test_counts)\n",
    "\n",
    "# Prediction\n",
    "predicted5 = S.predict(x_test_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     |       n     |\n",
      "     |   n   e   p |\n",
      "     |   e   u   o |\n",
      "     |   g   t   s |\n",
      "-----+-------------+\n",
      " neg |<377> 22  17 |\n",
      "neut | 116<174> 56 |\n",
      " pos |  41  41<169>|\n",
      "-----+-------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "0.690029615005\n",
      "stemmed: 0.71076011846\n"
     ]
    }
   ],
   "source": [
    "# View predictions\n",
    "test['predicted_RF']=predicted5\n",
    "\n",
    "testres = []\n",
    "predres = []\n",
    "for i in range(len(test)):\n",
    "    testres.append(test['target'][i])\n",
    "for i in range(len(test)):\n",
    "    predres.append(test['predicted_RF'][i])\n",
    "from nltk.metrics import ConfusionMatrix\n",
    "cm = nltk.ConfusionMatrix(testres, predres)\n",
    "print(cm)\n",
    "\n",
    "print (379+170+150)*1.00/len(test)\n",
    "print 'stemmed:', (377+174+169)*1.00/len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "dt = tree.DecisionTreeClassifier()\n",
    "\n",
    "NN = dt.fit(x_train_tfidf.toarray(), train['target'])\n",
    "\n",
    "## Prediction on test data\n",
    "# Tokenizing test phrase\n",
    "x_test_counts = count_vector.transform(test['data'])\n",
    "# Use transform() method to transform test count-matrix to 'tf-idf' representation\n",
    "x_test_tfidf = tfidf_transformer.transform(x_test_counts)\n",
    "\n",
    "# Prediction\n",
    "predicted5 = dt.predict(x_test_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     |       n     |\n",
      "     |   n   e   p |\n",
      "     |   e   u   o |\n",
      "     |   g   t   s |\n",
      "-----+-------------+\n",
      " neg |<332> 40  44 |\n",
      "neut | 120<158> 68 |\n",
      " pos |  40  54<157>|\n",
      "-----+-------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6386969397828233"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View predictions\n",
    "test['predicted_DT']=predicted5\n",
    "\n",
    "testres = []\n",
    "predres = []\n",
    "for i in range(len(test)):\n",
    "    testres.append(test['target'][i])\n",
    "for i in range(len(test)):\n",
    "    predres.append(test['predicted_DT'][i])\n",
    "from nltk.metrics import ConfusionMatrix\n",
    "cm = nltk.ConfusionMatrix(testres, predres)\n",
    "print(cm)\n",
    "\n",
    "(332+158+157)*1.00/len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "# now you can save it to a file\n",
    "with open('NewSGD_tfidf_uni.pkl', 'wb') as f:\n",
    "    pickle.dump(SG , f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
